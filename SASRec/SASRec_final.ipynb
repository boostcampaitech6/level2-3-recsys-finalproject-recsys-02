{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib3gvT2ilbxm"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DImYt6t4lbxm"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from google.oauth2 import service_account\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL0hmlgelbxm"
      },
      "source": [
        "## Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dDx1oOIDlbxm"
      },
      "outputs": [],
      "source": [
        "# model setting\n",
        "max_len = 10\n",
        "hidden_units = 50\n",
        "num_heads = 1\n",
        "num_layers = 2\n",
        "dropout_rate=0.5\n",
        "num_workers = 1\n",
        "device = 'cuda' # gpu 환경 확인 필요\n",
        "\n",
        "# training setting\n",
        "lr = 0.001\n",
        "batch_size = 128\n",
        "num_epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joV1Znhclbxm"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT_FILE = \"../Data/config/level3-416207-893f91c9529e.json\"  # 키 json 파일\n",
        "\n",
        "# Credentials 객체 생성\n",
        "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)\n",
        "\n",
        "# 빅쿼리 클라이언트 객체 생성\n",
        "project_id = \"level3-416207\"\n",
        "client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "\n",
        "# 쿼리 실행\n",
        "# 빅쿼리 디렉토리는 <프로젝트ID>.<데이터셋ID>.<테이블ID> 순으로 저장되어있음 ex) level3-416207.l3_30.l3_30\n",
        "QUERY = (\n",
        "    '''\n",
        "    SELECT *\n",
        "    FROM `level3-416207.log_129.ip_time_url_sorted_by_ip_time`\n",
        "    ''')\n",
        "\n",
        "\n",
        "# API request\n",
        "df = client.query(QUERY).to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df[['hashed_ip', 'local_time', 'request_url_endpoint']]\n",
        "df['local_time'] = pd.to_datetime(df['local_time'], format='%d/%b/%Y:%H:%M:%S')\n",
        "df['timestamp']=pd.to_datetime(df['local_time']).astype(int)//10**9\n",
        "df = df[['hashed_ip', 'request_url_endpoint', 'timestamp']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "spec = df[df['request_url_endpoint'].str.startswith('/spec')]\n",
        "comment = df[df['request_url_endpoint'].str.startswith('/comments')]\n",
        "product = df[df['request_url_endpoint'].str.startswith('/products')]\n",
        "\n",
        "df = pd.concat([spec, comment, product])\n",
        "\n",
        "df = df.sort_values(['hashed_ip', 'timestamp'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['product_id'] = df['request_url_endpoint'].str.split('/').str[2]\n",
        "df = df[df['product_id'].apply(lambda x: len(str(x)) == len('db696dbc255f0443bb7f782ac0ec24d45003f792cb9dbb7c810f7dd8216a18b2'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['user']=df['hashed_ip']\n",
        "df['item']=df['product_id']\n",
        "df['time']=df['timestamp']\n",
        "\n",
        "del df['hashed_ip'], df['request_url_endpoint'], df['timestamp'], df['product_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>item</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4807633</th>\n",
              "      <td>00012dd418cdd7e02ff8b1a54f0a8d69</td>\n",
              "      <td>63147c6e78fcf3360b76cb06745efa373dd7e08cbf92b7...</td>\n",
              "      <td>1698341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6746299</th>\n",
              "      <td>00012dd418cdd7e02ff8b1a54f0a8d69</td>\n",
              "      <td>63147c6e78fcf3360b76cb06745efa373dd7e08cbf92b7...</td>\n",
              "      <td>1698341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6212400</th>\n",
              "      <td>00012dd418cdd7e02ff8b1a54f0a8d69</td>\n",
              "      <td>63147c6e78fcf3360b76cb06745efa373dd7e08cbf92b7...</td>\n",
              "      <td>1698341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8831992</th>\n",
              "      <td>00012dd418cdd7e02ff8b1a54f0a8d69</td>\n",
              "      <td>f774892f67793bf7218b23c5b72ba7ee481f51d3dff647...</td>\n",
              "      <td>1698781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8877500</th>\n",
              "      <td>00012dd418cdd7e02ff8b1a54f0a8d69</td>\n",
              "      <td>f6abf3c612d5c21d0ab89f499c1a5629a0ae0d1627426c...</td>\n",
              "      <td>1698781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9915654</th>\n",
              "      <td>ffff620421fb3729200f091926ee2d3d</td>\n",
              "      <td>61c8319054f52d566790a25938cae12b87bd59e939765e...</td>\n",
              "      <td>1696971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9915653</th>\n",
              "      <td>ffff620421fb3729200f091926ee2d3d</td>\n",
              "      <td>61c8319054f52d566790a25938cae12b87bd59e939765e...</td>\n",
              "      <td>1696971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5669724</th>\n",
              "      <td>ffff620421fb3729200f091926ee2d3d</td>\n",
              "      <td>9dee36d244275b2cff23bf0c17971811acdedada16f26b...</td>\n",
              "      <td>1697565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7289093</th>\n",
              "      <td>ffff620421fb3729200f091926ee2d3d</td>\n",
              "      <td>9dee36d244275b2cff23bf0c17971811acdedada16f26b...</td>\n",
              "      <td>1697565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9680696</th>\n",
              "      <td>ffff620421fb3729200f091926ee2d3d</td>\n",
              "      <td>9dee36d244275b2cff23bf0c17971811acdedada16f26b...</td>\n",
              "      <td>1697566</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5754616 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     user  \\\n",
              "4807633  00012dd418cdd7e02ff8b1a54f0a8d69   \n",
              "6746299  00012dd418cdd7e02ff8b1a54f0a8d69   \n",
              "6212400  00012dd418cdd7e02ff8b1a54f0a8d69   \n",
              "8831992  00012dd418cdd7e02ff8b1a54f0a8d69   \n",
              "8877500  00012dd418cdd7e02ff8b1a54f0a8d69   \n",
              "...                                   ...   \n",
              "9915654  ffff620421fb3729200f091926ee2d3d   \n",
              "9915653  ffff620421fb3729200f091926ee2d3d   \n",
              "5669724  ffff620421fb3729200f091926ee2d3d   \n",
              "7289093  ffff620421fb3729200f091926ee2d3d   \n",
              "9680696  ffff620421fb3729200f091926ee2d3d   \n",
              "\n",
              "                                                      item     time  \n",
              "4807633  63147c6e78fcf3360b76cb06745efa373dd7e08cbf92b7...  1698341  \n",
              "6746299  63147c6e78fcf3360b76cb06745efa373dd7e08cbf92b7...  1698341  \n",
              "6212400  63147c6e78fcf3360b76cb06745efa373dd7e08cbf92b7...  1698341  \n",
              "8831992  f774892f67793bf7218b23c5b72ba7ee481f51d3dff647...  1698781  \n",
              "8877500  f6abf3c612d5c21d0ab89f499c1a5629a0ae0d1627426c...  1698781  \n",
              "...                                                    ...      ...  \n",
              "9915654  61c8319054f52d566790a25938cae12b87bd59e939765e...  1696971  \n",
              "9915653  61c8319054f52d566790a25938cae12b87bd59e939765e...  1696971  \n",
              "5669724  9dee36d244275b2cff23bf0c17971811acdedada16f26b...  1697565  \n",
              "7289093  9dee36d244275b2cff23bf0c17971811acdedada16f26b...  1697565  \n",
              "9680696  9dee36d244275b2cff23bf0c17971811acdedada16f26b...  1697566  \n",
              "\n",
              "[5754616 rows x 3 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# user별 interaction 개수를 계산\n",
        "user_interaction_counts = df['user'].value_counts()\n",
        "\n",
        "# interaction이 3개 이상인 user의 목록을 가져옴\n",
        "selected_users = user_interaction_counts[user_interaction_counts >= 3].index\n",
        "\n",
        "# interaction이 3개 이상인 user에 대한 데이터만 남김\n",
        "df = df[df['user'].isin(selected_users)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ2YmI3llbxn",
        "outputId": "75e3c02c-1903-404d-8f44-f66a2c5d3aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num users: 50217, num items: 36294\n"
          ]
        }
      ],
      "source": [
        "item_ids = df['item'].unique()\n",
        "user_ids = df['user'].unique()\n",
        "num_item, num_user = len(item_ids), len(user_ids)\n",
        "num_batch = num_user // batch_size\n",
        "\n",
        "# user, item indexing\n",
        "item2idx = pd.Series(data=np.arange(len(item_ids))+1, index=item_ids) # item re-indexing (1~num_item)\n",
        "user2idx = pd.Series(data=np.arange(len(user_ids)), index=user_ids) # user re-indexing (0~num_user-1)\n",
        "\n",
        "# dataframe indexing\n",
        "df = pd.merge(df, pd.DataFrame({'item': item_ids, 'item_idx': item2idx[item_ids].values}), on='item', how='inner')\n",
        "df = pd.merge(df, pd.DataFrame({'user': user_ids, 'user_idx': user2idx[user_ids].values}), on='user', how='inner')\n",
        "df.sort_values(['user_idx', 'time'], inplace=True)\n",
        "del df['item'], df['user']\n",
        "\n",
        "# train set, valid set 생성\n",
        "users = defaultdict(list) # defaultdict은 dictionary의 key가 없을때 default 값을 value로 반환\n",
        "user_train = {}\n",
        "user_valid = {}\n",
        "for u, i, t in zip(df['user_idx'], df['item_idx'], df['time']):\n",
        "    users[u].append(i)\n",
        "\n",
        "for user in users:\n",
        "    user_train[user] = users[user][:-1]\n",
        "    user_valid[user] = [users[user][-1]]\n",
        "\n",
        "print(f'num users: {num_user}, num items: {num_item}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Crj9yVT5lbxn"
      },
      "outputs": [],
      "source": [
        "# for training, data sampling\n",
        "def random_neg(l, r, s):\n",
        "    # log에 존재하는 아이템과 겹치지 않도록 sampling\n",
        "    t = np.random.randint(l, r)\n",
        "    while t in s:\n",
        "        t = np.random.randint(l, r)\n",
        "    return t\n",
        "\n",
        "def sample_batch(user_train, num_user, num_item, batch_size, max_len):\n",
        "    def sample():\n",
        "\n",
        "        user = np.random.randint(num_user)  # user를 임의로 선택\n",
        "\n",
        "        # 미리 max_len에 해당하는 array 생성, zero padding\n",
        "        seq = np.zeros([max_len], dtype=np.int32)\n",
        "        pos = np.zeros([max_len], dtype=np.int32)\n",
        "        neg = np.zeros([max_len], dtype=np.int32)\n",
        "        nxt = user_train[user][-1]\n",
        "        idx = max_len - 1\n",
        "\n",
        "        # negative sample은 train sequence에 없는 item 사용\n",
        "        train_item = set(user_train[user])\n",
        "        for i in reversed(user_train[user][:-1]):\n",
        "            # 미리 정의된 sequence를 역순으로 채움, ex: seq = [0,0,0,1,2,3] (0은 pad)\n",
        "            seq[idx] = i\n",
        "            pos[idx] = nxt\n",
        "            if nxt != 0:\n",
        "                neg[idx] = random_neg(1, num_item + 1, train_item)\n",
        "            nxt = i\n",
        "            idx -= 1\n",
        "            if idx == -1: break\n",
        "        return (user, seq, pos, neg)\n",
        "    user, seq, pos, neg = zip(*[sample() for _ in range(batch_size)])\n",
        "    user, seq, pos, neg = np.array(user), np.array(seq), np.array(pos), np.array(neg)\n",
        "    return user, seq, pos, neg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVLHK0oVlbxn"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "69zDNy6Albxo"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = nn.Dropout(dropout_rate) # TODO1: dropout rate를 hyper parameter로 사용하여 dropout layer를 구현하세요.\n",
        "\n",
        "    def forward(self, Q, K, V, mask):\n",
        "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units)\n",
        "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
        "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
        "        output = torch.matmul(attn_dist, V)  # dim of output : batchSize x num_head x seqLen x hidden_units\n",
        "        return output, attn_dist\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads # head의 수\n",
        "        self.hidden_units = hidden_units\n",
        "\n",
        "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
        "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate) # TODO2: ScaledDotProductAttention class를 사용하여 attention layer를 구현하세요.\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
        "\n",
        "    def forward(self, enc, mask):\n",
        "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
        "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
        "\n",
        "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
        "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
        "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
        "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
        "\n",
        "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
        "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)\n",
        "        output, attn_dist = self.attention(Q, K, V, mask)\n",
        "\n",
        "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "        output = output.view(batch_size, seqlen, -1)\n",
        "\n",
        "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
        "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual)\n",
        "        return output, attn_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lclDm0Yolbxp"
      },
      "source": [
        "### Position-wise Feed Forward Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lYp3RAIllbxp"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "\n",
        "        self.W_1 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.W_2 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n",
        "        output = self.layerNorm(self.dropout(output) + residual)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NUJ68SZglbxp"
      },
      "outputs": [],
      "source": [
        "class SASRecBlock(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(SASRecBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
        "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
        "\n",
        "    def forward(self, input_enc, mask):\n",
        "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
        "        output_enc = self.pointwise_feedforward(output_enc)\n",
        "        return output_enc, attn_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfIEkU3Elbxp"
      },
      "source": [
        "### SASRec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QTC2LUAglbxp"
      },
      "outputs": [],
      "source": [
        "class SASRec(nn.Module):\n",
        "    def __init__(self, num_user, num_item, hidden_units, num_heads, num_layers, maxlen, dropout_rate, device):\n",
        "        super(SASRec, self).__init__()\n",
        "\n",
        "        self.num_user = num_user\n",
        "        self.num_item = num_item\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.device = device\n",
        "\n",
        "        self.item_emb = nn.Embedding(num_item + 1, hidden_units, padding_idx=0) # TODO3: item embedding을 생성하세요. (padding index 고려 필요)\n",
        "        self.pos_emb = nn.Embedding(maxlen, hidden_units) # learnable positional encoding\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
        "\n",
        "        self.blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
        "\n",
        "    def feats(self, log_seqs):\n",
        "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.device))\n",
        "        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1])\n",
        "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device))\n",
        "        seqs = self.emb_layernorm(self.dropout(seqs))\n",
        "\n",
        "        # masking\n",
        "        mask_pad = torch.BoolTensor(log_seqs > 0).unsqueeze(1).unsqueeze(1) # TODO4: log_seqs=0인 경우 masking이 필요합니다. 해당 조건을 만족하는 mask를 구현하세요.\n",
        "        # 참고 unsqueeze operation을 통해 mask_pad와 mask_time의 차원을 맞춰주는 과정이 필요합니다.\n",
        "        mask_time = (1 - torch.triu(torch.ones((1, 1, seqs.size(1), seqs.size(1))), diagonal=1)).bool() # sequence의 순서를 고려\n",
        "        mask = (mask_pad & mask_time).to(self.device)\n",
        "        for block in self.blocks:\n",
        "            seqs, attn_dist = block(seqs, mask)\n",
        "        return seqs\n",
        "\n",
        "    def forward(self, log_seqs, pos_seqs, neg_seqs):\n",
        "        # 학습에 사용\n",
        "        feats = self.feats(log_seqs) # TODO5: Transformer를 사용해서 token 별 연산을 수행하세요.\n",
        "        pos_embs = self.item_emb(torch.LongTensor(pos_seqs).to(self.device))\n",
        "        neg_embs = self.item_emb(torch.LongTensor(neg_seqs).to(self.device))\n",
        "\n",
        "        pos_logits = (feats * pos_embs).sum(dim=-1)\n",
        "        neg_logits = (feats * neg_embs).sum(dim=-1)\n",
        "        return pos_logits, neg_logits\n",
        "\n",
        "    def predict(self, log_seqs, item_indices):\n",
        "        # evaluation에 사용\n",
        "        final_feats = self.feats(log_seqs)[:, -1, :]\n",
        "        item_embs = self.item_emb(torch.LongTensor(item_indices).to(self.device))\n",
        "        logits = item_embs.matmul(final_feats.unsqueeze(-1)).squeeze(-1)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi23gs2Klbxp"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7X8abJF4lbxp"
      },
      "outputs": [],
      "source": [
        "# setting\n",
        "model = SASRec(num_user, num_item, hidden_units, num_heads, num_layers, max_len, dropout_rate, device)\n",
        "model.to(device)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj2VxBPQlbxp",
        "outputId": "95f6e67f-4eb2-483e-8ab8-ba9553c89ac8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   1| Step: 391| Train loss: 1.73408: 100%|██████████| 392/392 [00:02<00:00, 132.22it/s]\n",
            "Epoch:   2| Step: 391| Train loss: 1.06506: 100%|██████████| 392/392 [00:02<00:00, 146.45it/s]\n",
            "Epoch:   3| Step: 391| Train loss: 1.04630: 100%|██████████| 392/392 [00:02<00:00, 157.52it/s]\n",
            "Epoch:   4| Step: 391| Train loss: 0.90911: 100%|██████████| 392/392 [00:02<00:00, 158.08it/s]\n",
            "Epoch:   5| Step: 391| Train loss: 0.88765: 100%|██████████| 392/392 [00:02<00:00, 158.44it/s]\n",
            "Epoch:   6| Step: 391| Train loss: 0.81259: 100%|██████████| 392/392 [00:02<00:00, 135.90it/s]\n",
            "Epoch:   7| Step: 391| Train loss: 0.71638: 100%|██████████| 392/392 [00:02<00:00, 145.97it/s]\n",
            "Epoch:   8| Step: 391| Train loss: 0.58331: 100%|██████████| 392/392 [00:02<00:00, 152.43it/s]\n",
            "Epoch:   9| Step: 391| Train loss: 0.48285: 100%|██████████| 392/392 [00:02<00:00, 152.23it/s]\n",
            "Epoch:  10| Step: 391| Train loss: 0.44762: 100%|██████████| 392/392 [00:02<00:00, 156.23it/s]\n",
            "Epoch:  11| Step: 391| Train loss: 0.36285: 100%|██████████| 392/392 [00:02<00:00, 155.15it/s]\n",
            "Epoch:  12| Step: 391| Train loss: 0.35424: 100%|██████████| 392/392 [00:02<00:00, 148.56it/s]\n",
            "Epoch:  13| Step: 391| Train loss: 0.38396: 100%|██████████| 392/392 [00:02<00:00, 146.97it/s]\n",
            "Epoch:  14| Step: 391| Train loss: 0.29329: 100%|██████████| 392/392 [00:02<00:00, 150.78it/s]\n",
            "Epoch:  15| Step: 391| Train loss: 0.31845: 100%|██████████| 392/392 [00:02<00:00, 155.22it/s]\n",
            "Epoch:  16| Step: 391| Train loss: 0.32209: 100%|██████████| 392/392 [00:02<00:00, 155.90it/s]\n",
            "Epoch:  17| Step: 391| Train loss: 0.32259: 100%|██████████| 392/392 [00:02<00:00, 148.54it/s]\n",
            "Epoch:  18| Step: 391| Train loss: 0.26164: 100%|██████████| 392/392 [00:02<00:00, 145.58it/s]\n",
            "Epoch:  19| Step: 391| Train loss: 0.32167: 100%|██████████| 392/392 [00:02<00:00, 148.78it/s]\n",
            "Epoch:  20| Step: 391| Train loss: 0.30463: 100%|██████████| 392/392 [00:02<00:00, 150.06it/s]\n",
            "Epoch:  21| Step: 391| Train loss: 0.28033: 100%|██████████| 392/392 [00:02<00:00, 145.09it/s]\n",
            "Epoch:  22| Step: 391| Train loss: 0.30358: 100%|██████████| 392/392 [00:02<00:00, 145.19it/s]\n",
            "Epoch:  23| Step: 391| Train loss: 0.30185: 100%|██████████| 392/392 [00:02<00:00, 151.24it/s]\n",
            "Epoch:  24| Step: 391| Train loss: 0.30127: 100%|██████████| 392/392 [00:02<00:00, 154.44it/s]\n",
            "Epoch:  25| Step: 391| Train loss: 0.29245: 100%|██████████| 392/392 [00:02<00:00, 155.38it/s]\n",
            "Epoch:  26| Step: 391| Train loss: 0.28178: 100%|██████████| 392/392 [00:02<00:00, 148.85it/s]\n",
            "Epoch:  27| Step: 391| Train loss: 0.29690: 100%|██████████| 392/392 [00:02<00:00, 156.56it/s]\n",
            "Epoch:  28| Step: 391| Train loss: 0.30524: 100%|██████████| 392/392 [00:02<00:00, 156.56it/s]\n",
            "Epoch:  29| Step: 391| Train loss: 0.29033: 100%|██████████| 392/392 [00:02<00:00, 155.85it/s]\n",
            "Epoch:  30| Step: 391| Train loss: 0.25547: 100%|██████████| 392/392 [00:02<00:00, 156.58it/s]\n",
            "Epoch:  31| Step: 391| Train loss: 0.27988: 100%|██████████| 392/392 [00:02<00:00, 156.03it/s]\n",
            "Epoch:  32| Step: 391| Train loss: 0.24133: 100%|██████████| 392/392 [00:02<00:00, 150.41it/s]\n",
            "Epoch:  33| Step: 391| Train loss: 0.28397: 100%|██████████| 392/392 [00:02<00:00, 152.05it/s]\n",
            "Epoch:  34| Step: 391| Train loss: 0.26131: 100%|██████████| 392/392 [00:02<00:00, 145.80it/s]\n",
            "Epoch:  35| Step: 391| Train loss: 0.25333: 100%|██████████| 392/392 [00:02<00:00, 145.11it/s]\n",
            "Epoch:  36| Step: 391| Train loss: 0.25537: 100%|██████████| 392/392 [00:02<00:00, 145.11it/s]\n",
            "Epoch:  37| Step: 391| Train loss: 0.24842: 100%|██████████| 392/392 [00:02<00:00, 146.79it/s]\n",
            "Epoch:  38| Step: 391| Train loss: 0.22156: 100%|██████████| 392/392 [00:02<00:00, 145.66it/s]\n",
            "Epoch:  39| Step: 391| Train loss: 0.29745: 100%|██████████| 392/392 [00:02<00:00, 145.37it/s]\n",
            "Epoch:  40| Step: 391| Train loss: 0.28119: 100%|██████████| 392/392 [00:02<00:00, 145.99it/s]\n",
            "Epoch:  41| Step: 391| Train loss: 0.27735: 100%|██████████| 392/392 [00:02<00:00, 145.01it/s]\n",
            "Epoch:  42| Step: 391| Train loss: 0.23769: 100%|██████████| 392/392 [00:02<00:00, 145.34it/s]\n",
            "Epoch:  43| Step: 391| Train loss: 0.25071: 100%|██████████| 392/392 [00:02<00:00, 145.05it/s]\n",
            "Epoch:  44| Step: 391| Train loss: 0.21335: 100%|██████████| 392/392 [00:02<00:00, 145.65it/s]\n",
            "Epoch:  45| Step: 391| Train loss: 0.24761: 100%|██████████| 392/392 [00:02<00:00, 146.07it/s]\n",
            "Epoch:  46| Step: 391| Train loss: 0.22132: 100%|██████████| 392/392 [00:02<00:00, 143.30it/s]\n",
            "Epoch:  47| Step: 391| Train loss: 0.21440: 100%|██████████| 392/392 [00:02<00:00, 144.79it/s]\n",
            "Epoch:  48| Step: 391| Train loss: 0.25598: 100%|██████████| 392/392 [00:02<00:00, 146.57it/s]\n",
            "Epoch:  49| Step: 391| Train loss: 0.19768: 100%|██████████| 392/392 [00:02<00:00, 144.08it/s]\n",
            "Epoch:  50| Step: 391| Train loss: 0.22530: 100%|██████████| 392/392 [00:02<00:00, 146.17it/s]\n"
          ]
        }
      ],
      "source": [
        "# training\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    tbar = tqdm(range(num_batch))\n",
        "    for step in tbar: # num_batch만큼 sampling\n",
        "        user, seq, pos, neg = sample_batch(user_train, num_user, num_item, batch_size, max_len)\n",
        "        pos_logits, neg_logits = model(seq, pos, neg)\n",
        "        pos_labels, neg_labels = torch.ones(pos_logits.shape, device=device), torch.zeros(neg_logits.shape, device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        indices = np.where(pos != 0)\n",
        "        loss = criterion(pos_logits[indices], pos_labels[indices])\n",
        "        loss += criterion(neg_logits[indices], neg_labels[indices])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Step: {step:3d}| Train loss: {loss:.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_Nf2KXclbxp"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sks4zbN_lbxp"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "NDCG = 0.0 # NDCG@10\n",
        "HIT = 0.0 # HIT@10\n",
        "\n",
        "num_item_sample = num_item\n",
        "num_user_sample = num_user\n",
        "users = np.random.randint(0, num_user, num_user_sample) # 1000개만 sampling 하여 evaluation\n",
        "for u in users:\n",
        "    seq = user_train[u][-max_len:]\n",
        "    rated = set(user_train[u] + user_valid[u])\n",
        "    item_idx = user_valid[u] + [random_neg(1, num_item + 1, rated) for _ in range(num_item_sample)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = -model.predict(np.array([seq]), np.array(item_idx))\n",
        "        predictions = predictions[0]\n",
        "        rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "    if rank < 10: # 만약 예측 성공시\n",
        "        NDCG += 1 / np.log2(rank + 2)\n",
        "        HIT += 1\n",
        "print(f'NDCG@10: {NDCG/num_user_sample}| HIT@10: {HIT/num_user_sample}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Recbole",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
