{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib3gvT2ilbxm"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DImYt6t4lbxm"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from google.oauth2 import service_account\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL0hmlgelbxm"
      },
      "source": [
        "## Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDx1oOIDlbxm"
      },
      "outputs": [],
      "source": [
        "# model setting\n",
        "max_len = 10\n",
        "hidden_units = 50\n",
        "num_heads = 1\n",
        "num_layers = 2\n",
        "dropout_rate=0.5\n",
        "num_workers = 1\n",
        "device = 'cuda' # gpu 환경 확인 필요\n",
        "\n",
        "# training setting\n",
        "lr = 0.001\n",
        "batch_size = 128\n",
        "num_epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joV1Znhclbxm"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT_FILE = \"../../Data/config/level3-416207-893f91c9529e.json\"  # 키 json 파일\n",
        "\n",
        "# Credentials 객체 생성\n",
        "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)\n",
        "\n",
        "# 빅쿼리 클라이언트 객체 생성\n",
        "project_id = \"level3-416207\"\n",
        "client = bigquery.Client(credentials=credentials, project=project_id)\n",
        "\n",
        "# 쿼리 실행\n",
        "# 빅쿼리 디렉토리는 <프로젝트ID>.<데이터셋ID>.<테이블ID> 순으로 저장되어있음 ex) level3-416207.l3_30.l3_30\n",
        "QUERY = (\n",
        "    '''\n",
        "    SELECT *\n",
        "    FROM `level3-416207.log_129.ip_time_url_sorted_by_ip_time`\n",
        "    ''')\n",
        "\n",
        "\n",
        "# API request\n",
        "df = client.query(QUERY).to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df[['hashed_ip', 'local_time', 'request_url_endpoint']]\n",
        "df['local_time'] = pd.to_datetime(df['local_time'], format='%d/%b/%Y:%H:%M:%S')\n",
        "df['timestamp']=pd.to_datetime(df['local_time']).astype(int)//10**9\n",
        "df = df[['hashed_ip', 'request_url_endpoint', 'timestamp']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "spec = df[df['request_url_endpoint'].str.startswith('/spec')]\n",
        "comment = df[df['request_url_endpoint'].str.startswith('/comments')]\n",
        "product = df[df['request_url_endpoint'].str.startswith('/products')]\n",
        "\n",
        "df = pd.concat([product])\n",
        "#df = pd.concat([spec, comment, product])\n",
        "\n",
        "df = df.sort_values(['hashed_ip', 'timestamp'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['product_id'] = df['request_url_endpoint'].str.split('/').str[2]\n",
        "df = df[df['product_id'].apply(lambda x: len(str(x)) == len('db696dbc255f0443bb7f782ac0ec24d45003f792cb9dbb7c810f7dd8216a18b2'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['user']=df['hashed_ip']\n",
        "df['item']=df['product_id']\n",
        "df['time']=df['timestamp']\n",
        "\n",
        "del df['hashed_ip'], df['request_url_endpoint'], df['timestamp'], df['product_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>item</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6277936</th>\n",
              "      <td>00012dd418cdd7e02ff8b1a54f0a8d69</td>\n",
              "      <td>63147c6e78fcf3360b76cb06745efa373dd7e08cbf92b7...</td>\n",
              "      <td>1698341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5067189</th>\n",
              "      <td>00012dd418cdd7e02ff8b1a54f0a8d69</td>\n",
              "      <td>f774892f67793bf7218b23c5b72ba7ee481f51d3dff647...</td>\n",
              "      <td>1698781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5117086</th>\n",
              "      <td>00012dd418cdd7e02ff8b1a54f0a8d69</td>\n",
              "      <td>f6abf3c612d5c21d0ab89f499c1a5629a0ae0d1627426c...</td>\n",
              "      <td>1698781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6098168</th>\n",
              "      <td>00012dd418cdd7e02ff8b1a54f0a8d69</td>\n",
              "      <td>92bc40d09c604fb8dbb2d283959e0c4b2ba98b984eac1b...</td>\n",
              "      <td>1698782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4848645</th>\n",
              "      <td>00012dd418cdd7e02ff8b1a54f0a8d69</td>\n",
              "      <td>6fdae04380f97c32942d34657e0ca2f5853c7dd02557ce...</td>\n",
              "      <td>1702406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9403146</th>\n",
              "      <td>fffd7e8a1ab612e28dc13fe8f544379e</td>\n",
              "      <td>0dd8b052665d7eba05a212ed2945c10fe360239420285e...</td>\n",
              "      <td>1704570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9850286</th>\n",
              "      <td>fffee8de197f26f89f965c2ea16c3ccd</td>\n",
              "      <td>4e625ee7f03e937a876d5462ba2c06fed229000c303fb4...</td>\n",
              "      <td>1699133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6994109</th>\n",
              "      <td>ffff620421fb3729200f091926ee2d3d</td>\n",
              "      <td>5fa517a74273fb46a3dbd5224cb7e49c8a6011c767f703...</td>\n",
              "      <td>1696896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8098253</th>\n",
              "      <td>ffff620421fb3729200f091926ee2d3d</td>\n",
              "      <td>5fa517a74273fb46a3dbd5224cb7e49c8a6011c767f703...</td>\n",
              "      <td>1696896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10013957</th>\n",
              "      <td>ffff620421fb3729200f091926ee2d3d</td>\n",
              "      <td>61c8319054f52d566790a25938cae12b87bd59e939765e...</td>\n",
              "      <td>1696971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1955788 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      user  \\\n",
              "6277936   00012dd418cdd7e02ff8b1a54f0a8d69   \n",
              "5067189   00012dd418cdd7e02ff8b1a54f0a8d69   \n",
              "5117086   00012dd418cdd7e02ff8b1a54f0a8d69   \n",
              "6098168   00012dd418cdd7e02ff8b1a54f0a8d69   \n",
              "4848645   00012dd418cdd7e02ff8b1a54f0a8d69   \n",
              "...                                    ...   \n",
              "9403146   fffd7e8a1ab612e28dc13fe8f544379e   \n",
              "9850286   fffee8de197f26f89f965c2ea16c3ccd   \n",
              "6994109   ffff620421fb3729200f091926ee2d3d   \n",
              "8098253   ffff620421fb3729200f091926ee2d3d   \n",
              "10013957  ffff620421fb3729200f091926ee2d3d   \n",
              "\n",
              "                                                       item     time  \n",
              "6277936   63147c6e78fcf3360b76cb06745efa373dd7e08cbf92b7...  1698341  \n",
              "5067189   f774892f67793bf7218b23c5b72ba7ee481f51d3dff647...  1698781  \n",
              "5117086   f6abf3c612d5c21d0ab89f499c1a5629a0ae0d1627426c...  1698781  \n",
              "6098168   92bc40d09c604fb8dbb2d283959e0c4b2ba98b984eac1b...  1698782  \n",
              "4848645   6fdae04380f97c32942d34657e0ca2f5853c7dd02557ce...  1702406  \n",
              "...                                                     ...      ...  \n",
              "9403146   0dd8b052665d7eba05a212ed2945c10fe360239420285e...  1704570  \n",
              "9850286   4e625ee7f03e937a876d5462ba2c06fed229000c303fb4...  1699133  \n",
              "6994109   5fa517a74273fb46a3dbd5224cb7e49c8a6011c767f703...  1696896  \n",
              "8098253   5fa517a74273fb46a3dbd5224cb7e49c8a6011c767f703...  1696896  \n",
              "10013957  61c8319054f52d566790a25938cae12b87bd59e939765e...  1696971  \n",
              "\n",
              "[1955788 rows x 3 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# user별 interaction 개수를 계산\n",
        "user_interaction_counts = df['user'].value_counts()\n",
        "\n",
        "# interaction이 3개 이상인 user의 목록을 가져옴\n",
        "selected_users = user_interaction_counts[user_interaction_counts >= 3].index\n",
        "\n",
        "# interaction이 3개 이상인 user에 대한 데이터만 남김\n",
        "df = df[df['user'].isin(selected_users)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ2YmI3llbxn",
        "outputId": "75e3c02c-1903-404d-8f44-f66a2c5d3aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num users: 33075, num items: 35088\n"
          ]
        }
      ],
      "source": [
        "item_ids = df['item'].unique()\n",
        "user_ids = df['user'].unique()\n",
        "num_item, num_user = len(item_ids), len(user_ids)\n",
        "num_batch = num_user // batch_size\n",
        "\n",
        "# user, item indexing\n",
        "item2idx = pd.Series(data=np.arange(len(item_ids))+1, index=item_ids) # item re-indexing (1~num_item)\n",
        "user2idx = pd.Series(data=np.arange(len(user_ids)), index=user_ids) # user re-indexing (0~num_user-1)\n",
        "\n",
        "# dataframe indexing\n",
        "df = pd.merge(df, pd.DataFrame({'item': item_ids, 'item_idx': item2idx[item_ids].values}), on='item', how='inner')\n",
        "df = pd.merge(df, pd.DataFrame({'user': user_ids, 'user_idx': user2idx[user_ids].values}), on='user', how='inner')\n",
        "df.sort_values(['user_idx', 'time'], inplace=True)\n",
        "del df['item'], df['user']\n",
        "\n",
        "# train set, valid set 생성\n",
        "users = defaultdict(list) # defaultdict은 dictionary의 key가 없을때 default 값을 value로 반환\n",
        "user_train = {}\n",
        "user_valid = {}\n",
        "for u, i, t in zip(df['user_idx'], df['item_idx'], df['time']):\n",
        "    users[u].append(i)\n",
        "\n",
        "for user in users:\n",
        "    user_train[user] = users[user][:-1]\n",
        "    user_valid[user] = [users[user][-1]]\n",
        "\n",
        "print(f'num users: {num_user}, num items: {num_item}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Crj9yVT5lbxn"
      },
      "outputs": [],
      "source": [
        "# for training, data sampling\n",
        "def random_neg(l, r, s):\n",
        "    # log에 존재하는 아이템과 겹치지 않도록 sampling\n",
        "    t = np.random.randint(l, r)\n",
        "    while t in s:\n",
        "        t = np.random.randint(l, r)\n",
        "    return t\n",
        "\n",
        "def sample_batch(user_train, num_user, num_item, batch_size, max_len):\n",
        "    def sample():\n",
        "\n",
        "        user = np.random.randint(num_user)  # user를 임의로 선택\n",
        "\n",
        "        # 미리 max_len에 해당하는 array 생성, zero padding\n",
        "        seq = np.zeros([max_len], dtype=np.int32)\n",
        "        pos = np.zeros([max_len], dtype=np.int32)\n",
        "        neg = np.zeros([max_len], dtype=np.int32)\n",
        "        nxt = user_train[user][-1]\n",
        "        idx = max_len - 1\n",
        "\n",
        "        # negative sample은 train sequence에 없는 item 사용\n",
        "        train_item = set(user_train[user])\n",
        "        for i in reversed(user_train[user][:-1]):\n",
        "            # 미리 정의된 sequence를 역순으로 채움, ex: seq = [0,0,0,1,2,3] (0은 pad)\n",
        "            seq[idx] = i\n",
        "            pos[idx] = nxt\n",
        "            if nxt != 0:\n",
        "                neg[idx] = random_neg(1, num_item + 1, train_item)\n",
        "            nxt = i\n",
        "            idx -= 1\n",
        "            if idx == -1: break\n",
        "        return (user, seq, pos, neg)\n",
        "    user, seq, pos, neg = zip(*[sample() for _ in range(batch_size)])\n",
        "    user, seq, pos, neg = np.array(user), np.array(seq), np.array(pos), np.array(neg)\n",
        "    return user, seq, pos, neg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVLHK0oVlbxn"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "69zDNy6Albxo"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout = nn.Dropout(dropout_rate) # TODO1: dropout rate를 hyper parameter로 사용하여 dropout layer를 구현하세요.\n",
        "\n",
        "    def forward(self, Q, K, V, mask):\n",
        "        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units)\n",
        "        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n",
        "        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n",
        "        output = torch.matmul(attn_dist, V)  # dim of output : batchSize x num_head x seqLen x hidden_units\n",
        "        return output, attn_dist\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads # head의 수\n",
        "        self.hidden_units = hidden_units\n",
        "\n",
        "        # query, key, value, output 생성을 위해 Linear 모델 생성\n",
        "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate) # TODO2: ScaledDotProductAttention class를 사용하여 attention layer를 구현하세요.\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
        "\n",
        "    def forward(self, enc, mask):\n",
        "        residual = enc # residual connection을 위해 residual 부분을 저장\n",
        "        batch_size, seqlen = enc.size(0), enc.size(1)\n",
        "\n",
        "        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n",
        "        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
        "        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
        "        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units)\n",
        "\n",
        "        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n",
        "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)\n",
        "        output, attn_dist = self.attention(Q, K, V, mask)\n",
        "\n",
        "        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "        output = output.view(batch_size, seqlen, -1)\n",
        "\n",
        "        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n",
        "        output = self.layerNorm(self.dropout(self.W_O(output)) + residual)\n",
        "        return output, attn_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lclDm0Yolbxp"
      },
      "source": [
        "### Position-wise Feed Forward Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lYp3RAIllbxp"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "\n",
        "        self.W_1 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.W_2 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n",
        "        output = self.layerNorm(self.dropout(output) + residual)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NUJ68SZglbxp"
      },
      "outputs": [],
      "source": [
        "class SASRecBlock(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_units, dropout_rate):\n",
        "        super(SASRecBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n",
        "        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n",
        "\n",
        "    def forward(self, input_enc, mask):\n",
        "        output_enc, attn_dist = self.attention(input_enc, mask)\n",
        "        output_enc = self.pointwise_feedforward(output_enc)\n",
        "        return output_enc, attn_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfIEkU3Elbxp"
      },
      "source": [
        "### SASRec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QTC2LUAglbxp"
      },
      "outputs": [],
      "source": [
        "class SASRec(nn.Module):\n",
        "    def __init__(self, num_user, num_item, hidden_units, num_heads, num_layers, maxlen, dropout_rate, device):\n",
        "        super(SASRec, self).__init__()\n",
        "\n",
        "        self.num_user = num_user\n",
        "        self.num_item = num_item\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.device = device\n",
        "\n",
        "        self.item_emb = nn.Embedding(num_item + 1, hidden_units, padding_idx=0) # TODO3: item embedding을 생성하세요. (padding index 고려 필요)\n",
        "        self.pos_emb = nn.Embedding(maxlen, hidden_units) # learnable positional encoding\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n",
        "\n",
        "        self.blocks = nn.ModuleList([SASRecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n",
        "\n",
        "    def feats(self, log_seqs):\n",
        "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.device))\n",
        "        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1])\n",
        "        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device))\n",
        "        seqs = self.emb_layernorm(self.dropout(seqs))\n",
        "\n",
        "        # masking\n",
        "        mask_pad = torch.BoolTensor(log_seqs > 0).unsqueeze(1).unsqueeze(1) # TODO4: log_seqs=0인 경우 masking이 필요합니다. 해당 조건을 만족하는 mask를 구현하세요.\n",
        "        # 참고 unsqueeze operation을 통해 mask_pad와 mask_time의 차원을 맞춰주는 과정이 필요합니다.\n",
        "        mask_time = (1 - torch.triu(torch.ones((1, 1, seqs.size(1), seqs.size(1))), diagonal=1)).bool() # sequence의 순서를 고려\n",
        "        mask = (mask_pad & mask_time).to(self.device)\n",
        "        for block in self.blocks:\n",
        "            seqs, attn_dist = block(seqs, mask)\n",
        "        return seqs\n",
        "\n",
        "    def forward(self, log_seqs, pos_seqs, neg_seqs):\n",
        "        # 학습에 사용\n",
        "        feats = self.feats(log_seqs) # TODO5: Transformer를 사용해서 token 별 연산을 수행하세요.\n",
        "        pos_embs = self.item_emb(torch.LongTensor(pos_seqs).to(self.device))\n",
        "        neg_embs = self.item_emb(torch.LongTensor(neg_seqs).to(self.device))\n",
        "\n",
        "        pos_logits = (feats * pos_embs).sum(dim=-1)\n",
        "        neg_logits = (feats * neg_embs).sum(dim=-1)\n",
        "        return pos_logits, neg_logits\n",
        "\n",
        "    def predict(self, log_seqs, item_indices):\n",
        "        # evaluation에 사용\n",
        "        final_feats = self.feats(log_seqs)[:, -1, :]\n",
        "        item_embs = self.item_emb(torch.LongTensor(item_indices).to(self.device))\n",
        "        logits = item_embs.matmul(final_feats.unsqueeze(-1)).squeeze(-1)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi23gs2Klbxp"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7X8abJF4lbxp"
      },
      "outputs": [],
      "source": [
        "# setting\n",
        "model = SASRec(num_user, num_item, hidden_units, num_heads, num_layers, max_len, dropout_rate, device)\n",
        "model.to(device)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj2VxBPQlbxp",
        "outputId": "95f6e67f-4eb2-483e-8ab8-ba9553c89ac8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   1| Step: 257| Train loss: 3.02935: 100%|██████████| 258/258 [00:01<00:00, 130.88it/s]\n",
            "Epoch:   2| Step: 257| Train loss: 1.61470: 100%|██████████| 258/258 [00:01<00:00, 151.96it/s]\n",
            "Epoch:   3| Step: 257| Train loss: 1.31541: 100%|██████████| 258/258 [00:01<00:00, 148.32it/s]\n",
            "Epoch:   4| Step: 257| Train loss: 1.31490: 100%|██████████| 258/258 [00:01<00:00, 148.44it/s]\n",
            "Epoch:   5| Step: 257| Train loss: 1.18575: 100%|██████████| 258/258 [00:01<00:00, 148.66it/s]\n",
            "Epoch:   6| Step: 257| Train loss: 1.11722: 100%|██████████| 258/258 [00:01<00:00, 149.25it/s]\n",
            "Epoch:   7| Step: 257| Train loss: 1.02641: 100%|██████████| 258/258 [00:01<00:00, 147.83it/s]\n",
            "Epoch:   8| Step: 257| Train loss: 0.98343: 100%|██████████| 258/258 [00:01<00:00, 149.94it/s]\n",
            "Epoch:   9| Step: 257| Train loss: 0.85962: 100%|██████████| 258/258 [00:01<00:00, 149.59it/s]\n",
            "Epoch:  10| Step: 257| Train loss: 0.89668: 100%|██████████| 258/258 [00:01<00:00, 149.11it/s]\n",
            "Epoch:  11| Step: 257| Train loss: 0.77085: 100%|██████████| 258/258 [00:01<00:00, 149.55it/s]\n",
            "Epoch:  12| Step: 257| Train loss: 0.68885: 100%|██████████| 258/258 [00:01<00:00, 149.39it/s]\n",
            "Epoch:  13| Step: 257| Train loss: 0.67219: 100%|██████████| 258/258 [00:01<00:00, 149.55it/s]\n",
            "Epoch:  14| Step: 257| Train loss: 0.62497: 100%|██████████| 258/258 [00:01<00:00, 149.10it/s]\n",
            "Epoch:  15| Step: 257| Train loss: 0.62716: 100%|██████████| 258/258 [00:01<00:00, 149.44it/s]\n",
            "Epoch:  16| Step: 257| Train loss: 0.62547: 100%|██████████| 258/258 [00:01<00:00, 148.25it/s]\n",
            "Epoch:  17| Step: 257| Train loss: 0.55407: 100%|██████████| 258/258 [00:01<00:00, 149.24it/s]\n",
            "Epoch:  18| Step: 257| Train loss: 0.59320: 100%|██████████| 258/258 [00:01<00:00, 148.68it/s]\n",
            "Epoch:  19| Step: 257| Train loss: 0.53318: 100%|██████████| 258/258 [00:01<00:00, 148.88it/s]\n",
            "Epoch:  20| Step: 257| Train loss: 0.53883: 100%|██████████| 258/258 [00:01<00:00, 146.33it/s]\n",
            "Epoch:  21| Step: 257| Train loss: 0.53609: 100%|██████████| 258/258 [00:01<00:00, 148.83it/s]\n",
            "Epoch:  22| Step: 257| Train loss: 0.55761: 100%|██████████| 258/258 [00:01<00:00, 155.17it/s]\n",
            "Epoch:  23| Step: 257| Train loss: 0.54523: 100%|██████████| 258/258 [00:01<00:00, 160.49it/s]\n",
            "Epoch:  24| Step: 257| Train loss: 0.55449: 100%|██████████| 258/258 [00:01<00:00, 160.67it/s]\n",
            "Epoch:  25| Step: 257| Train loss: 0.49960: 100%|██████████| 258/258 [00:01<00:00, 160.52it/s]\n",
            "Epoch:  26| Step: 257| Train loss: 0.51866: 100%|██████████| 258/258 [00:01<00:00, 159.29it/s]\n",
            "Epoch:  27| Step: 257| Train loss: 0.50469: 100%|██████████| 258/258 [00:01<00:00, 159.71it/s]\n",
            "Epoch:  28| Step: 257| Train loss: 0.58747: 100%|██████████| 258/258 [00:01<00:00, 160.52it/s]\n",
            "Epoch:  29| Step: 257| Train loss: 0.52545: 100%|██████████| 258/258 [00:01<00:00, 159.87it/s]\n",
            "Epoch:  30| Step: 257| Train loss: 0.56317: 100%|██████████| 258/258 [00:01<00:00, 159.71it/s]\n",
            "Epoch:  31| Step: 257| Train loss: 0.51022: 100%|██████████| 258/258 [00:01<00:00, 159.24it/s]\n",
            "Epoch:  32| Step: 257| Train loss: 0.45941: 100%|██████████| 258/258 [00:01<00:00, 159.62it/s]\n",
            "Epoch:  33| Step: 257| Train loss: 0.51991: 100%|██████████| 258/258 [00:01<00:00, 160.49it/s]\n",
            "Epoch:  34| Step: 257| Train loss: 0.55897: 100%|██████████| 258/258 [00:01<00:00, 158.84it/s]\n",
            "Epoch:  35| Step: 257| Train loss: 0.54402: 100%|██████████| 258/258 [00:01<00:00, 159.88it/s]\n",
            "Epoch:  36| Step: 257| Train loss: 0.52161: 100%|██████████| 258/258 [00:01<00:00, 160.54it/s]\n",
            "Epoch:  37| Step: 257| Train loss: 0.54269: 100%|██████████| 258/258 [00:01<00:00, 160.73it/s]\n",
            "Epoch:  38| Step: 257| Train loss: 0.50564: 100%|██████████| 258/258 [00:01<00:00, 160.60it/s]\n",
            "Epoch:  39| Step: 257| Train loss: 0.48310: 100%|██████████| 258/258 [00:01<00:00, 161.03it/s]\n",
            "Epoch:  40| Step: 257| Train loss: 0.48677: 100%|██████████| 258/258 [00:01<00:00, 160.90it/s]\n",
            "Epoch:  41| Step: 257| Train loss: 0.51955: 100%|██████████| 258/258 [00:01<00:00, 160.55it/s]\n",
            "Epoch:  42| Step: 257| Train loss: 0.53276: 100%|██████████| 258/258 [00:01<00:00, 160.64it/s]\n",
            "Epoch:  43| Step: 257| Train loss: 0.57432: 100%|██████████| 258/258 [00:01<00:00, 160.57it/s]\n",
            "Epoch:  44| Step: 257| Train loss: 0.48728: 100%|██████████| 258/258 [00:01<00:00, 160.37it/s]\n",
            "Epoch:  45| Step: 257| Train loss: 0.46777: 100%|██████████| 258/258 [00:01<00:00, 159.72it/s]\n",
            "Epoch:  46| Step: 257| Train loss: 0.51657: 100%|██████████| 258/258 [00:01<00:00, 160.23it/s]\n",
            "Epoch:  47| Step: 257| Train loss: 0.55004: 100%|██████████| 258/258 [00:01<00:00, 160.24it/s]\n",
            "Epoch:  48| Step: 257| Train loss: 0.52240: 100%|██████████| 258/258 [00:01<00:00, 160.10it/s]\n",
            "Epoch:  49| Step: 257| Train loss: 0.46156: 100%|██████████| 258/258 [00:01<00:00, 160.30it/s]\n",
            "Epoch:  50| Step: 257| Train loss: 0.56527: 100%|██████████| 258/258 [00:01<00:00, 159.70it/s]\n"
          ]
        }
      ],
      "source": [
        "# training\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    tbar = tqdm(range(num_batch))\n",
        "    for step in tbar: # num_batch만큼 sampling\n",
        "        user, seq, pos, neg = sample_batch(user_train, num_user, num_item, batch_size, max_len)\n",
        "        pos_logits, neg_logits = model(seq, pos, neg)\n",
        "        pos_labels, neg_labels = torch.ones(pos_logits.shape, device=device), torch.zeros(neg_logits.shape, device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        indices = np.where(pos != 0)\n",
        "        loss = criterion(pos_logits[indices], pos_labels[indices])\n",
        "        loss += criterion(neg_logits[indices], neg_labels[indices])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Step: {step:3d}| Train loss: {loss:.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_Nf2KXclbxp"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sks4zbN_lbxp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG@10: 0.19438740744031202| HIT@10: 0.287\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "NDCG5 = 0.0 # NDCG@5\n",
        "HIT5 = 0.0 # HIT@5\n",
        "NDCG10 = 0.0 # NDCG@10\n",
        "HIT10 = 0.0 # HIT@10\n",
        "num_item_sample = num_item\n",
        "num_user_sample = num_user\n",
        "users = np.random.randint(0, num_user, num_user_sample) # 1000개만 sampling 하여 evaluation\n",
        "for u in users:\n",
        "    seq = user_train[u][-max_len:]\n",
        "    rated = set(user_train[u] + user_valid[u])\n",
        "    item_idx = user_valid[u] + [random_neg(1, num_item + 1, rated) for _ in range(num_item_sample)]\n",
        "    with torch.no_grad():\n",
        "        predictions = -model.predict(np.array([seq]), np.array(item_idx))\n",
        "        predictions = predictions[0]\n",
        "        rank = predictions.argsort().argsort()[0].item()\n",
        "    if rank < 5: # 만약 예측 성공시\n",
        "        NDCG5 += 1 / np.log2(rank + 2)\n",
        "        HIT5 += 1\n",
        "    if rank < 10: # 만약 예측 성공시\n",
        "        NDCG10 += 1 / np.log2(rank + 2)\n",
        "        HIT10 += 1\n",
        "print(f'NDCG@5: {NDCG5/num_user_sample}| HIT@5: {HIT5/num_user_sample}')\n",
        "print(f'NDCG@10: {NDCG10/num_user_sample}| HIT@10: {HIT10/num_user_sample}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Recbole",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
