{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/test/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from box import Box\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    ###### TopicModeling  ######\n",
    "    'num_topics':20,\n",
    "    'random_state':42,\n",
    "    'passes':10,\n",
    "    \n",
    "    ###### lightgcn  ######\n",
    "    'num_epochs' : 150,\n",
    "    \"reg\" : 1e-5,\n",
    "    'lr' : 0.0001,\n",
    "    \"emb_dim\" : 20,\n",
    "    \"n_layers\" : 3,\n",
    "    'batch_size' : 50,\n",
    "    \"node_dropout\" : 0.2,\n",
    "    'valid_samples' : 2, # 검증에 사용할 sample 수\n",
    "    'seed' : 22,\n",
    "    'n_batch' : 10,\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = Box(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df[df['uri_first']==1]\n",
    "    df['timestamp']=pd.to_datetime(df['local_time']).astype(int)//10**9\n",
    "    df = df[['hashed_ip', 'products', 'timestamp']]\n",
    "\n",
    "    df['user']=df['hashed_ip']\n",
    "    df['item']=df['products']\n",
    "    df['time']=df['timestamp']\n",
    "\n",
    "    df.sort_values(['user', 'timestamp'])\n",
    "\n",
    "    del df['hashed_ip'], df['products'], df['timestamp']\n",
    "    user_interaction_counts = df['user'].value_counts()\n",
    "    selected_users = user_interaction_counts[user_interaction_counts >= 5].index\n",
    "    df = df[df['user'].isin(selected_users)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(): \n",
    "    # LOAD ITEM2IDX PICKLE\n",
    "    \n",
    "    SERVICE_ACCOUNT_FILE = \"/home/user/level3-416207-893f91c9529e (1).json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)\n",
    "    project_id = \"level3-416207\"\n",
    "    storage_client = storage.Client(credentials=credentials, project=project_id)\n",
    "    bucket_name = 'crwalnoti'\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    item2idx_name = '240320/item_to_idx.pickle'\n",
    "    inter_name = '240320/inter_240129.csv'\n",
    "\n",
    "    # prepare item2idx\n",
    "    blob_item2idx = bucket.blob(item2idx_name)\n",
    "    with blob_item2idx.open(mode='rb') as f:\n",
    "        item2idx = pickle.load(f)\n",
    "    \n",
    "    # prepare interaction_df\n",
    "    blob_inter = bucket.blob(inter_name)\n",
    "    with blob_inter.open(mode='rb') as f:\n",
    "        interaction_df = pd.read_csv(f)\n",
    "        \n",
    "    interaction_df = preprocess(interaction_df)\n",
    "\n",
    "    return item2idx, interaction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TopicModeling_DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeTMDataSet():\n",
    "    def __init__(self):\n",
    "        self.item2idx , self.df = load_data()\n",
    "        \n",
    "        self.df[\"item_idx\"] = self.df[\"item\"].map(self.item2idx)   \n",
    "        self.df['item_name'] = self.df['item'].map(self.item2name())\n",
    "        \n",
    "        # inter_dict & df user 순서 주의\n",
    "        self.inter_dict = self.df.groupby('user', sort=False)['item_name'].apply(set).apply(list).to_dict()\n",
    "        self.user_ids = list(self.inter_dict.keys())\n",
    "        self.user2idx = {user_id: index for index, user_id in enumerate(self.user_ids)}\n",
    "        \n",
    "        self.df[\"user_idx\"] = self.df[\"user\"].map(self.user2idx)\n",
    "        \n",
    "        self.num_item, self.num_user = len(self.item2idx), len(self.user2idx)\n",
    "        \n",
    "        self.dictionary, self.corpus = self.TM_traindata()\n",
    "        \n",
    "    def item2name(self):\n",
    "        with open('/home/user/pickle/product_info_df.pickle', 'rb') as fr:\n",
    "            product_info = pickle.load(fr)\n",
    "            \n",
    "        product_data = product_info.copy()\n",
    "        product_data['title'] = product_data['title'].map(lambda x: x.replace(\"'\",'').replace(',','').replace('(', ' ').replace(')', ' '))\n",
    "        product_data['title'] = product_data['title'].map(lambda x: x.lower())\n",
    "        product_data['title'] = product_data['title'].map(lambda x: x.split(' '))\n",
    "        product_data['title'] = product_data['title'].map(lambda x: ' '.join(x).split())\n",
    "        product_data['title'] = product_data['title'].map(lambda x: ' '.join(x))\n",
    "        \n",
    "        dict_products = product_data[['id','title']].set_index('id').to_dict()['title']\n",
    "        \n",
    "        return dict_products\n",
    "    \n",
    "    def TM_traindata(self):\n",
    "        documents = list(self.inter_dict.values())\n",
    "        dictionary = Dictionary(documents)\n",
    "        corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "        return dictionary, corpus\n",
    "    \n",
    "    def get_dictionary(self):\n",
    "        return self.dictionary\n",
    "    \n",
    "    def get_corpus(self):\n",
    "        return self.corpus\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1442948/855873563.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['timestamp']=pd.to_datetime(df['local_time']).astype(int)//10**9\n"
     ]
    }
   ],
   "source": [
    "TM_dataset = MakeTMDataSet()\n",
    "dictionary = TM_dataset.get_dictionary()\n",
    "corpus = TM_dataset.get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(empty_values): 0\n"
     ]
    }
   ],
   "source": [
    "# 빈 리스트인 value가 존재하는지 확인\n",
    "empty_values = {user_id: item_list for user_id, item_list in TM_dataset.inter_dict.items() if len(item_list) == 0}\n",
    "print(f\"len(empty_values): {len(empty_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model=LdaModel(corpus=corpus, id2word=dictionary, \n",
    "                   num_topics=config.num_topics, \n",
    "                   random_state=config.random_state, \n",
    "                   passes=config.passes )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check User Topic Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_topic_vectors = [lda_model.get_document_topics(bow) for bow in corpus]\n",
    "user_topic_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의 user vector 확인\n",
    "doc_id = 153  # 사용자 ID\n",
    "doc_bow = corpus[doc_id]\n",
    "doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
    "\n",
    "print(f\"Document #{doc_id} Topics:\")\n",
    "for topic, prob in doc_topics:\n",
    "    print(f\"Topic {topic}: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling's VECTOR --> LightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeLightGCNDataSet():\n",
    "    def __init__(self, TM_dataset, lda_model, config):\n",
    "        self.config = config\n",
    "        self.TM_dataset = TM_dataset\n",
    "        self.lda_model = lda_model\n",
    "        \n",
    "        self.df = self.TM_dataset.df\n",
    "        self.user2idx = self.TM_dataset.user2idx\n",
    "        self.item2idx = self.TM_dataset.item2idx\n",
    "        self.num_user, self.num_item = self.TM_dataset.num_user, self.TM_dataset.num_item\n",
    "        \n",
    "        self.exist_users = [i for i in range(self.num_user)]\n",
    "        self.exist_items = [i for i in range(self.num_item)]\n",
    "        \n",
    "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
    "        self.R_train, self.R_valid, self.R_total = self.generate_dok_matrix()\n",
    "        self.ngcf_adj_matrix = self.generate_ngcf_adj_matrix()\n",
    "        \n",
    "        self.user_topic_tensor = self.get_TM_user_vector()\n",
    "        \n",
    "        self.n_train = len(self.R_train)\n",
    "        self.batch_size = self.config.batch_size\n",
    "        \n",
    "    def generate_sequence_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        split train/valid\n",
    "        중복 허용\n",
    "        \"\"\"\n",
    "        users = defaultdict(list)\n",
    "        user_train = {}\n",
    "        user_valid = {}\n",
    "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['time']):\n",
    "            users[user].append(item)\n",
    "        \n",
    "        for user in users:\n",
    "            np.random.seed(self.config.seed)\n",
    "            user_total = users[user]\n",
    "            valid_indices = random.sample(range(len(user_total)), 2)\n",
    "            valid = [user_total[idx] for idx in valid_indices]\n",
    "            train = [user_total[idx] for idx in range(len(user_total)) if idx not in valid_indices]\n",
    "            user_train[user] = train\n",
    "            user_valid[user] = valid\n",
    "        \n",
    "        return user_train, user_valid\n",
    "    \n",
    "    def generate_dok_matrix(self):\n",
    "        R_train = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        R_valid = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        R_total = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
    "        user_list = self.exist_users   # user2idx에 있는 value값\n",
    "        for user in user_list:\n",
    "            train_items = self.user_train[user]\n",
    "            valid_items = self.user_valid[user]\n",
    "            \n",
    "            for train_item in train_items:\n",
    "                R_train[user, train_item] = 1.0\n",
    "                R_total[user, train_item] = 1.0\n",
    "            \n",
    "            for valid_item in valid_items:\n",
    "                R_valid[user, valid_item] = 1.0\n",
    "                R_total[user, valid_item] = 1.0\n",
    "        \n",
    "        return R_train, R_valid, R_total\n",
    "\n",
    "    def generate_ngcf_adj_matrix(self):\n",
    "        adj_mat = sp.dok_matrix((self.num_user + self.num_item, self.num_user + self.num_item), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil() # to_list\n",
    "        R = self.R_train.tolil()\n",
    "\n",
    "        adj_mat[:self.num_user, self.num_user:] = R\n",
    "        adj_mat[self.num_user:, :self.num_user] = R.T\n",
    "        adj_mat = adj_mat.todok() # to_dok_matrix\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "            d_inv = np.power(rowsum, -.5).flatten()  \n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "            norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)\n",
    "\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        ngcf_adj_matrix = normalized_adj_single(adj_mat)\n",
    "        return ngcf_adj_matrix.tocsr()\n",
    "\n",
    "    def get_TM_user_vector(self):\n",
    "        user_topic_matrix = np.zeros((self.num_user, self.config.num_topics))\n",
    "        corpus = self.TM_dataset.get_corpus()\n",
    "        \n",
    "        user_topic_vectors = [self.lda_model.get_document_topics(bow, minimum_probability=0.0) \n",
    "                              for bow in corpus]\n",
    "        for i, user_vec in enumerate(user_topic_vectors):\n",
    "            \"\"\"\n",
    "                i: user idx\n",
    "                user_vec: (topic, prob)\n",
    "            \"\"\"\n",
    "            for topic, prob in user_vec:\n",
    "                user_topic_matrix[i, topic] = prob\n",
    "\n",
    "        # numpy array --> torch tensor\n",
    "        user_topic_tensor = torch.tensor(user_topic_matrix, dtype=torch.float32)\n",
    "        \n",
    "        return user_topic_tensor\n",
    "\n",
    "    def sampling(self):\n",
    "        users = random.sample(self.exist_users, self.config.batch_size)\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.user_train[u]\n",
    "            pos_batch = random.sample(pos_items, num)\n",
    "            return pos_batch\n",
    "        \n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = list(set(self.exist_items) - set(self.user_train[u]))\n",
    "            neg_batch = random.sample(neg_items, num)\n",
    "            return neg_batch\n",
    "        \n",
    "        pos_items, neg_items = [], []\n",
    "        for user in users:\n",
    "            pos_items += sample_pos_items_for_u(user, 1)\n",
    "            neg_items += sample_neg_items_for_u(user, 1)\n",
    "        \n",
    "        return users, pos_items, neg_items\n",
    "        \n",
    "    def get_train_valid_data(self):\n",
    "        return self.user_train, self.user_valid\n",
    "\n",
    "    def get_R_data(self):\n",
    "        return self.R_train, self.R_valid, self.R_total\n",
    "\n",
    "    def get_ngcf_adj_matrix_data(self):\n",
    "        return self.ngcf_adj_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim, n_layers, reg, node_dropout, adj_mtx, user_topic_tensor):\n",
    "        super().__init__()\n",
    "        # initialize Class attributes\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.l = adj_mtx   # 인접 행렬, user-item interaction\n",
    "        self.user_topic_tensor = user_topic_tensor.to(device)\n",
    "        \n",
    "        # PyTorch sparse tensor\n",
    "        self.graph = self._convert_sp_mat_to_sp_tensor(self.l)\n",
    "\n",
    "        self.reg = reg   # regularization\n",
    "        self.n_layers = n_layers   # num of conv layers\n",
    "        self.node_dropout = node_dropout\n",
    "\n",
    "        # Initialize weights\n",
    "        # TM + Xavier\n",
    "        self.weight_dict = self._init_weights()\n",
    "        print(\"Weights initialized.\")\n",
    "\n",
    "    def _init_weights(self):\n",
    "        print(\"Initializing weights...\")\n",
    "        weight_dict = nn.ParameterDict() \n",
    "\n",
    "        initializer = torch.nn.init.xavier_uniform_\n",
    "        \n",
    "        weight_dict['user_embedding'] = nn.Parameter(self.user_topic_tensor)\n",
    "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\n",
    "\n",
    "        return weight_dict\n",
    "\n",
    "    # convert sparse matrix into sparse PyTorch tensor\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"\n",
    "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        X = Adjacency matrix, scipy sparse matrix\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        i = torch.LongTensor(np.mat([coo.row, coo.col]))   # 0이 아닌 원소의 위치 정보\n",
    "        v = torch.FloatTensor(coo.data)   # 원소 값 정보\n",
    "        # coo.shape: 원본 sparse matrix shape\n",
    "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n",
    "        return res\n",
    "\n",
    "    # apply node_dropout\n",
    "    def _droupout_sparse(self, X):\n",
    "        \"\"\"\n",
    "        Drop individual locations in X\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        X = adjacency matrix (PyTorch sparse tensor)\n",
    "        dropout = fraction of nodes to drop\n",
    "        noise_shape = number of non non-zero entries of X\n",
    "        \"\"\"\n",
    "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(device)\n",
    "        i = X.coalesce().indices()   # 0이 아닌 원소 위치\n",
    "        v = X.coalesce()._values()   # 0이 아닌 요소 값\n",
    "        i[:,node_dropout_mask] = 0   # masking\n",
    "        v[node_dropout_mask] = 0\n",
    "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\n",
    "\n",
    "        return  X_dropout.mul(1/(1-self.node_dropout))\n",
    "\n",
    "    def forward(self, u, i, j):\n",
    "        \"\"\"\n",
    "        Computes the forward pass\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        u = user\n",
    "        i = positive item (user interacted with item)\n",
    "        j = negative item (user did not interact with item)\n",
    "        \"\"\"\n",
    "        # apply drop-out mask\n",
    "        graph = self._droupout_sparse(self.graph) if self.node_dropout > 0 else self.graph\n",
    "\n",
    "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\n",
    "\n",
    "        for k in range(self.n_layers):\n",
    "            ego_embeddings = torch.sparse.mm(graph, ego_embeddings)\n",
    "        \n",
    "        u_emb, i_emb = ego_embeddings.split([self.n_users, self.n_items], 0)\n",
    "\n",
    "        self.u_emb = u_emb\n",
    "        self.i_emb = i_emb\n",
    "        \n",
    "        u_emb_batch = u_emb[u]  # (batch_size, emb_dim)\n",
    "        pos_i_emb_batch = i_emb[i]  # (batch_size, emb_dim)\n",
    "        neg_i_emb_batch = i_emb[j]  # (batch_size, emb_dim)\n",
    "        \n",
    "\n",
    "        # pos_scores & neg_scores\n",
    "        pos_scores = torch.sum(u_emb_batch * pos_i_emb_batch, dim=1)  # (batch_size)\n",
    "        neg_scores = torch.sum(u_emb_batch * neg_i_emb_batch, dim=1)  # (batch_size)\n",
    "        \n",
    "        # Concatenate pos & neg\n",
    "        scores = torch.cat([pos_scores, neg_scores])  # (2*batch_size)\n",
    "        labels = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)])  # (2*batch_size)\n",
    "\n",
    "        # Calculate BCEWithLogitsLoss\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = loss_fn(scores, labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1442948/3039804438.py:76: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -.5).flatten()\n"
     ]
    }
   ],
   "source": [
    "lightgcn_dataset=MakeLightGCNDataSet(TM_dataset, lda_model, config)\n",
    "ngcf_adj_matrix = lightgcn_dataset.get_ngcf_adj_matrix_data()\n",
    "R_train, R_valid, R_total = lightgcn_dataset.get_R_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights...\n",
      "Weights initialized.\n"
     ]
    }
   ],
   "source": [
    "model = LightGCN(\n",
    "    n_users = lightgcn_dataset.num_user,\n",
    "    n_items = lightgcn_dataset.num_item,\n",
    "    emb_dim = config.emb_dim,\n",
    "    n_layers = config.n_layers,\n",
    "    reg = config.reg,\n",
    "    node_dropout = config.node_dropout,\n",
    "    adj_mtx = ngcf_adj_matrix,\n",
    "    user_topic_tensor = lightgcn_dataset.user_topic_tensor,\n",
    "    ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, make_graph_data_set, optimizer, n_batch):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for step in range(1, n_batch + 1):\n",
    "        user, pos, neg = make_graph_data_set.sampling()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(user, pos, neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.item()\n",
    "    loss_val /= n_batch\n",
    "    return loss_val\n",
    "\n",
    "def split_matrix(X, n_splits=10):\n",
    "    splits = []\n",
    "    chunk_size = X.shape[0] // n_splits\n",
    "    for i in range(n_splits):\n",
    "        start = i * chunk_size\n",
    "        end = X.shape[0] if i == n_splits - 1 else (i + 1) * chunk_size\n",
    "        splits.append(X[start:end])\n",
    "    return splits\n",
    "\n",
    "def compute_ndcg_k(pred_items, test_items, test_indices, k):\n",
    "    \n",
    "    r = (test_items * pred_items).gather(1, test_indices)\n",
    "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().to(device)\n",
    "    \n",
    "    dcg = (r[:, :k]/f).sum(1)                                               \n",
    "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)   \n",
    "    ndcg = dcg/dcg_max                                                     \n",
    "    \n",
    "    ndcg[torch.isnan(ndcg)] = 0\n",
    "    return ndcg\n",
    "\n",
    "def evaluate(u_emb, i_emb, Rtr, Rte, k = 10):\n",
    "\n",
    "    # split matrices\n",
    "    ue_splits = split_matrix(u_emb)\n",
    "    tr_splits = split_matrix(Rtr)\n",
    "    te_splits = split_matrix(Rte)\n",
    "\n",
    "    recall_k, ndcg_k= [], []\n",
    "    # compute results for split matrices\n",
    "    for ue_f, tr_f, te_f in zip(ue_splits, tr_splits, te_splits):\n",
    "\n",
    "        scores = torch.mm(ue_f, i_emb.t())\n",
    "\n",
    "        test_items = torch.from_numpy(te_f.todense()).float().to(device)\n",
    "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().to(device)\n",
    "        scores = scores * non_train_items\n",
    "\n",
    "        _, test_indices = torch.topk(scores, dim=1, k=k)\n",
    "        \n",
    "        pred_items = torch.zeros_like(scores).float()\n",
    "        pred_items.scatter_(dim=1, index=test_indices, src=torch.ones_like(test_indices).float().to(device))\n",
    "\n",
    "        topk_preds = torch.zeros_like(scores).float()\n",
    "        topk_preds.scatter_(dim=1, index=test_indices[:, :k], src=torch.ones_like(test_indices).float())\n",
    "        \n",
    "        TP = (test_items * topk_preds).sum(1)                      \n",
    "        rec = TP/test_items.sum(1)\n",
    "   \n",
    "        ndcg = compute_ndcg_k(pred_items, test_items, test_indices, k)\n",
    "\n",
    "        recall_k.append(rec)\n",
    "        ndcg_k.append(ndcg)\n",
    "\n",
    "    return torch.cat(ndcg_k).mean(), torch.cat(recall_k).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hit = 0\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    tbar = tqdm(range(1))\n",
    "    for _ in tbar:\n",
    "        train_loss = train(\n",
    "            model = model, \n",
    "            make_graph_data_set = lightgcn_dataset, \n",
    "            optimizer = optimizer,\n",
    "            n_batch = config.n_batch,\n",
    "            )\n",
    "        with torch.no_grad():\n",
    "            ndcg, hit = evaluate(\n",
    "                u_emb = model.u_emb.detach(), \n",
    "                i_emb = model.i_emb.detach(), \n",
    "                Rtr = R_train, \n",
    "                Rte = R_valid, \n",
    "                k = 10,\n",
    "                )\n",
    "        # if best_hit < hit:\n",
    "        #     best_hit = hit\n",
    "        #     torch.save(model.state_dict(), os.path.join(config.model_path, config.model_name))\n",
    "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a6f8b45af1aebb989f3a6bef1d64b68316b6214ed3f64b3b1b4e0eb810a54a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
