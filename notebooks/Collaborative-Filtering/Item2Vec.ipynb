{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import normal_\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../asset/inter_240129.csv')\n",
    "df = df[df['local_time'] >= '2024-02-05 00:00:00+00:00']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data = pd.read_csv('../asset/item.csv')\n",
    "product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_idx = {ip:idx for idx, ip in enumerate(df['hashed_ip'].unique())}\n",
    "idx_to_user = {idx:ip for idx, ip in enumerate(df['hashed_ip'].unique())}\n",
    "item_to_idx = {pid:idx for idx, pid in enumerate(product_data['id'].unique())}\n",
    "idx_to_item = {idx:pid for idx, pid in enumerate(product_data['id'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashed_ip'] = df['hashed_ip'].map(user_to_idx)\n",
    "df['products'] = df['products'].map(item_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## positive sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상품이 최소 min_count 회 이상 등장해야 학습되고 모델에 저장됨\n",
    "min_count = 3\n",
    "# 1개의 positive sample당 negative sample의 개수\n",
    "negative = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = df.drop(columns='local_time', axis=0)\n",
    "positive_samples = positive_samples.drop_duplicates(keep='first', ignore_index=True)\n",
    "\n",
    "product_ids_for_training = list()\n",
    "for pid in tqdm(positive_samples['products'].unique()):\n",
    "    if positive_samples[positive_samples['products'] == pid].shape[0] >= min_count:\n",
    "        product_ids_for_training.append(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_positive_samples = dict()\n",
    "new_positive_samples['hashed_ip'] = list()\n",
    "new_positive_samples['w_pid'] = list()\n",
    "new_positive_samples['c_pid'] = list()\n",
    "\n",
    "user_negative_samples = dict()\n",
    "\n",
    "product_ids = df['products'].unique()\n",
    "\n",
    "for ip in tqdm(df['hashed_ip'].unique()):\n",
    "    user_positive_samples = positive_samples[positive_samples['hashed_ip'] == ip]\n",
    "    user_products = user_positive_samples['products'].tolist()\n",
    "    # sampling을 위해 각 user id의 negative products를 저장\n",
    "    user_neg_products = [pid for pid in product_ids if pid not in user_products]\n",
    "    user_negative_samples[ip] = np.array(user_neg_products)\n",
    "    for w_pid in user_products:\n",
    "        # 단어가 최소 등장 횟수를 만족하지 않음\n",
    "        if w_pid not in product_ids_for_training:\n",
    "            continue\n",
    "        for c_pid in user_products:\n",
    "            if c_pid == w_pid:\n",
    "                continue\n",
    "            new_positive_samples['hashed_ip'].append(ip)\n",
    "            new_positive_samples['w_pid'].append(w_pid)\n",
    "            new_positive_samples['c_pid'].append(c_pid)\n",
    "\n",
    "new_positive_samples = pd.DataFrame(new_positive_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_positive_samples['hashed_ip'] = new_positive_samples['hashed_ip'].astype(\"category\")\n",
    "new_positive_samples['w_pid'] = new_positive_samples['w_pid'].astype(\"category\")\n",
    "new_positive_samples['c_pid'] = new_positive_samples['c_pid'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    new_positive_samples, random_state=seed, test_size=0.20, #stratify=new_positive_samples['hashed_ip'], \n",
    ")\n",
    "print('학습 데이터 크기:', train_df.shape)\n",
    "print('테스트 데이터 크기:', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch의 DataLoader에서 사용할 수 있도록 변환\n",
    "train_dataset = TensorDataset(torch.LongTensor(np.array(train_df)))\n",
    "test_dataset = TensorDataset(torch.LongTensor(np.array(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Negative_Sampler(nn.Module):\n",
    "    \"\"\"\n",
    "    Negative Sampler\n",
    "\n",
    "    Args:\n",
    "        - user_negative_samples: (Dict) keys: user id, items: list of negative samples\n",
    "        - n_negs: (int) negative sample의 수\n",
    "    Shape:\n",
    "        - Input: (torch.Tensor) user id들. Shape: (batch size,)\n",
    "        - Output: (torch.Tensor) sampling된 negative samples. Shape: (batch size, n_negs)\n",
    "    \"\"\"\n",
    "    def __init__(self, user_negative_samples, n_negs):\n",
    "        super(Negative_Sampler, self).__init__()\n",
    "        self.user_negative_samples = user_negative_samples\n",
    "        self.n_negs = n_negs\n",
    "\n",
    "    def forward(self, user_ids):\n",
    "        user_ids = user_ids.to('cpu').numpy()\n",
    "        negative_samples = np.array([np.random.choice(a=self.user_negative_samples[user_id], size=self.n_negs,replace=False) for user_id in user_ids])\n",
    "        return torch.from_numpy(negative_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGNS class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNS(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip-Gram with Negative Sampling\n",
    "\n",
    "    Args:\n",
    "        - n_items: (int) 전체 아이템의 수\n",
    "        - emb_dim: (int) Embedding의 Dimension\n",
    "        - user_negative_samples: (Dict) hashed_ip 별 전체 negative sample\n",
    "        - n_negs: (int) negative sample의 수\n",
    "    Shape:\n",
    "        - Input: (torch.Tensor) input features, (hashed_ip, 중심 item id, 주변 item id). Shape: (batch size, 3)\n",
    "        - Output: (torch.Tensor) sampling된 negative samples와 positive sample의 Loss 합. Shape: ()\n",
    "    \"\"\"\n",
    "    def __init__(self, n_items, emb_dim, user_negative_samples, n_negs ,device=torch.device(\"cpu\")):\n",
    "        super(SGNS, self).__init__()\n",
    "\n",
    "        # initialize Class attributes\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.user_negative_samples = user_negative_samples\n",
    "        self.n_negs = n_negs\n",
    "        self.negative_sampler = Negative_Sampler(self.user_negative_samples, self.n_negs)\n",
    "\n",
    "        # define embeddings\n",
    "        # 중심 아이템\n",
    "        self.w_item_embedding = nn.Embedding(self.n_items, self.emb_dim)\n",
    "        # 주변 아이템\n",
    "        self.c_item_embedding = nn.Embedding(self.n_items, self.emb_dim)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    # initialize weights\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            normal_(module.weight.data, mean=0.0, std=0.01)\n",
    "\n",
    "    def forward(self, input_feature):\n",
    "        batch_size = input_feature.size()[0]\n",
    "\n",
    "        user_ids, w_item, c_item = torch.split(input_feature, [1, 1, 1], -1)\n",
    "        # 유저 id\n",
    "        user_ids = user_ids.squeeze(-1)\n",
    "        # 중심 아이템\n",
    "        w_item = w_item.squeeze(-1)\n",
    "        # 주변 아이템 (positive sample)\n",
    "        c_item = c_item.squeeze(-1)\n",
    "        # 주변 아이템 negative sampling\n",
    "        neg_c_items = self.negative_sampler(user_ids).to(self.device) \n",
    "\n",
    "        # 중심 아이템 embedding\n",
    "        w_item_e = self.w_item_embedding(w_item).to(self.device) \n",
    "        # 주변 아이템 (positive sample) embedding\n",
    "        c_item_e = self.c_item_embedding(c_item).to(self.device) \n",
    "        # 주변 아이템 (negative sample) embedding\n",
    "        neg_c_items_e = self.c_item_embedding(neg_c_items).to(self.device) \n",
    "\n",
    "        w_item_e = w_item_e.view(batch_size, 1, self.emb_dim)\n",
    "        c_item_e = c_item_e.view(batch_size, self.emb_dim, 1)\n",
    "        neg_c_items_e = neg_c_items_e.permute(0, 2, 1)\n",
    "\n",
    "        pos_output = torch.bmm(w_item_e, c_item_e)\n",
    "        pos_output = pos_output.squeeze(-1)\n",
    "        pos_output = self.sigmoid(pos_output).squeeze(-1)\n",
    "\n",
    "        pos_y = torch.ones(pos_output.size()).to(self.device)\n",
    "        pos_loss = self.loss_fn(pos_output, pos_y)\n",
    "\n",
    "        neg_output = torch.bmm(w_item_e, neg_c_items_e) \n",
    "        neg_output = neg_output.squeeze(-1)\n",
    "        neg_output = self.sigmoid(neg_output)\n",
    "\n",
    "        neg_y = torch.zeros(neg_output.size()).to(self.device)\n",
    "        neg_loss = self.loss_fn(neg_output, neg_y)\n",
    "\n",
    "        return pos_loss + neg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch, (X,) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        # Compute prediction and loss\n",
    "        loss = model(X)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if (batch+1) % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1) * len(X)\n",
    "            print(f\"Loss: {loss:>7f} | [{current:>5d}/{size:>5d}]\")\n",
    "    train_loss /= num_batches\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss= 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, in dataloader:\n",
    "            X= X.to(device)\n",
    "            loss = model(X)\n",
    "            test_loss += loss.item()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error:\\n\\tAvg Loss: {test_loss:>8f}\")\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def train_and_test(train_dataloader, test_dataloader, model, optimizer, epochs):\n",
    "    train_loss, test_loss = list(), list()\n",
    "\n",
    "    for t in tqdm(range(epochs)):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_result= train_loop(train_dataloader, model, optimizer)\n",
    "        train_loss.append(train_result)\n",
    "        test_result = test_loop(test_dataloader, model)\n",
    "        test_loss.append(test_result)\n",
    "        print(\"-------------------------------\\n\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Hyperparameter ########\n",
    "\n",
    "batch_size = 2048\n",
    "data_shuffle = True\n",
    "emb_dim = 512\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "gpu_idx = 0\n",
    "\n",
    "n_items = product_data['id'].nunique()\n",
    "\n",
    "################################\n",
    "#torch.cuda.empty_cache() # if necessary\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda:{}\".format(gpu_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=data_shuffle)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=data_shuffle)\n",
    "\n",
    "model = SGNS(n_items, emb_dim, user_negative_samples, negative, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, test_loss = train_and_test(train_dataloader, test_dataloader, model, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(\n",
    "    model: SGNS,\n",
    "    product_id: int,\n",
    "    n: int,\n",
    "    n_products: int\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    해당 product id와 cosine similarity가 가장 높은 n개의 product id를 반환\n",
    "\n",
    "    :param model: (SGNS) SGNS model\n",
    "    :param product_id: (int) 기준이 되는 product id\n",
    "    :param n: (int) 반환하는 product의 수\n",
    "    :param n_products: (int) 전체 product의 수\n",
    "    :return: (List) n개의 product id list\n",
    "    \"\"\"\n",
    "    input_tensor = torch.tensor(product_id).to('cuda')\n",
    "    product_embedding = model.w_item_embedding(input_tensor)\n",
    "    product_embedding = product_embedding.squeeze(-1)\n",
    "    similarity_list = list()\n",
    "    for pid in range(n_products):\n",
    "        if pid == product_id:\n",
    "            continue\n",
    "        p_tensor = torch.tensor(pid).to('cuda')\n",
    "        p_embedding = model.w_item_embedding(p_tensor)\n",
    "        p_embedding = p_embedding.squeeze(-1)\n",
    "        similarity = torch.nn.CosineSimilarity(dim=0)(product_embedding, p_embedding)\n",
    "        similarity_list.append((pid, similarity.item()))\n",
    "    similarity_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return similarity_list[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "product_id = 22\n",
    "n_products = df['products'].nunique()\n",
    "\n",
    "print(product_data[product_data['id']==idx_to_item[product_id]]['title'])\n",
    "\n",
    "similar_products = get_most_similar(model, product_id, n, n_products)\n",
    "for similar_pid, similarity in similar_products:\n",
    "    title = product_data[product_data['id']==idx_to_item[similar_pid]]['title']\n",
    "\n",
    "    print(f'\\ntitle: {title} | cosine similarity: {similarity}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
